{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from statistics import mean\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPercent=0.8\n",
    "ValidationPercent=0.1\n",
    "TestPercent=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data Preprocessing\n",
    "#Generate Data from file\n",
    "def GenerateProcessData(sp,dp,hofd):\n",
    "    Pdata=[]\n",
    "    P1data=[]\n",
    "    Tdata=[]\n",
    "    Fdata=[]\n",
    "    #object for file\n",
    "    with open(hofd,'rU') as feature:\n",
    "        read3=csv.reader(feature)\n",
    "        #loop for each row of feature\n",
    "        for row in read3:\n",
    "            datarow=[]\n",
    "            #loop for each column in a row\n",
    "            for column in row:\n",
    "                datarow.append(column)\n",
    "            Fdata.append(datarow)\n",
    "    #reading same and different pair file\n",
    "    samepair = pd.read_csv(sp)\n",
    "    diffpair=pd.read_csv(dp)\n",
    "    #shuffling pair file for random selection\n",
    "    samepair=samepair.sample(frac=1).reset_index(drop=True)\n",
    "    diffpair=diffpair.sample(frac=1).reset_index(drop=True)\n",
    "    #Selecting equal number of pair\n",
    "    samepair=samepair[:70000]\n",
    "    diffpair=diffpair[:70000]\n",
    "    #concatenating same and different pair\n",
    "    finalpair=pd.concat([samepair,diffpair])\n",
    "    #Shufflig the final pair for random selection\n",
    "    finalpair=finalpair.sample(frac=1).reset_index(drop=True)\n",
    "    #addving values to list\n",
    "    Pdata=finalpair.values\n",
    "    print(Pdata.shape)\n",
    "    #extract Target\n",
    "    Tdata=Pdata[:,2]\n",
    "    Tdata=np.array(Tdata,dtype=float)\n",
    "    \n",
    "    #delete target from the pair list\n",
    "    Pdata=np.delete(Pdata,[2],axis=1)\n",
    "    #Delete title row and column\n",
    "    Fdata=np.delete(Fdata,[0],axis=0)\n",
    "    #Fdata=np.delete(Fdata,[0],axis=1)\n",
    "    return Pdata, Tdata, Fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for concatenation\n",
    "def ConcateFeature(Pdata,Fdata):\n",
    "    Dict=[]\n",
    "    Dict1=[]\n",
    "    for row in Pdata:\n",
    "        for row1 in Fdata:\n",
    "            #searching Writer 1 in feature dataset\n",
    "            if row[0] == row1[0]:\n",
    "                Dict.append(row1)\n",
    "            #searching Writer 2 in feature dataset\n",
    "            elif row[1] == row1[0]:\n",
    "                Dict1.append(row1)\n",
    "    #deleting Writer name from writer 1 \n",
    "    print(len(Dict[0]))\n",
    "    Dict=np.delete(Dict,[0],axis=1)\n",
    "    print(len(Dict[0]))\n",
    "    Dict=[[float(j) for j in i]for i in Dict]\n",
    "    #deleting Writer name from writer 2 \n",
    "    Dict1=np.delete(Dict1,[0],axis=1)\n",
    "    Dict1=[[float(j) for j in i]for i in Dict1]\n",
    "    #concatenating both writer features\n",
    "    Dict=np.hstack((Dict,Dict1))\n",
    "    Dict=np.transpose(Dict)\n",
    "    return Dict\n",
    "#function for subtraction\n",
    "def SubstractFeature(Pdata,Fdata):\n",
    "    Dict=[]\n",
    "    Dict1=[]\n",
    "    result=[]\n",
    "    for row in Pdata:\n",
    "        for row1 in Fdata:\n",
    "            if row[0] == row1[0]:\n",
    "                #searching Writer 1 in feature dataset\n",
    "                Dict.append(row1)\n",
    "            elif row[1] == row1[0]:\n",
    "                #searching Writer 2 in feature dataset\n",
    "                Dict1.append(row1)\n",
    "    #deleting Writer name from writer 1 \n",
    "    Dict=np.delete(Dict,[0],axis=1)\n",
    "    Dict=[[float(j) for j in i]for i in Dict]\n",
    "    #deleting Writer name from writer 2\n",
    "    Dict1=np.delete(Dict1,[0],axis=1)\n",
    "    Dict1=[[float(j) for j in i]for i in Dict1]\n",
    "    Dict=np.subtract(Dict,Dict1)\n",
    "    Dict=np.transpose(Dict)\n",
    "    return Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Diving dataset into training, validation and testing\n",
    "#Generate Data for training, validation and testing\n",
    "def GenerateData(data,percent,UsedData):\n",
    "    #if data for training\n",
    "    if (UsedData==0):\n",
    "        lenght=int(math.ceil(len(data[0])*percent))\n",
    "        data=data[:,0:lenght]\n",
    "    #data for testing and validation\n",
    "    elif (UsedData!=0):\n",
    "        lenght=int(math.ceil(len(data[0])*percent))\n",
    "        end=UsedData+lenght\n",
    "        data=data[:,UsedData+1:end]\n",
    "    data=np.transpose(data)\n",
    "    return data\n",
    "#Generate Target for training, validation and testing\n",
    "def GenerateTarget(target,percent,UsedData):\n",
    "    #target for testing\n",
    "    if UsedData==0 :\n",
    "        lenght=int(math.ceil(len(target)*percent))\n",
    "        target=target[0:lenght]\n",
    "    #target for testing and validation\n",
    "    elif UsedData!=0 :\n",
    "        lenght=int(math.ceil(len(target)*percent))\n",
    "        end=UsedData+lenght\n",
    "        target=target[UsedData+1:end]\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: 'U' mode is deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 3)\n",
      "[['0113a_num1.png' '0113a_num2.png']\n",
      " ['0005b_num1.png' '0005c_num5.png']\n",
      " ['0562a_num4.png' '0562a_num5.png']\n",
      " ...\n",
      " ['0354a_num1.png' '0354b_num3.png']\n",
      " ['0630a_num1.png' '0633a_num3.png']\n",
      " ['0078b_num6.png' '0078c_num1.png']]\n",
      "(140000,)\n",
      "[['0001a_num1.png' '0' '0' ... '0' '0' '0']\n",
      " ['0001a_num2.png' '0' '0' ... '0' '0' '0']\n",
      " ['0001a_num3.png' '0' '0' ... '0' '0' '0']\n",
      " ...\n",
      " ['1568a_num3.png' '0' '0' ... '0' '0' '0']\n",
      " ['1568c_num1.png' '0' '0' ... '0' '0' '0']\n",
      " ['1568c_num2.png' '0' '0' ... '0' '0' '0']]\n"
     ]
    }
   ],
   "source": [
    "#processing dataset\n",
    "Pairs, Target, Features = GenerateProcessData('samepairs.csv','differentpairs.csv','GSCFD.csv')\n",
    "print(Pairs)\n",
    "print(Target.shape)\n",
    "print(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n",
      "512\n",
      "(1024, 140000)\n"
     ]
    }
   ],
   "source": [
    "#function for concatenation\n",
    "#comment next two lines and remove # after that for substraction\n",
    "ConcatFeat=ConcateFeature(Pairs,Features)\n",
    "print(ConcatFeat.shape)\n",
    "#function for Subtraction\n",
    "#SubFeat=SubstractFeature(Pairs,Features)\n",
    "#print(Sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 1024)\n",
      "(112000,)\n"
     ]
    }
   ],
   "source": [
    "#generating training data and target\n",
    "#Change ConcatFeat to SubFeat for subtraction\n",
    "TrainData=GenerateData(ConcatFeat,TrainingPercent,0)\n",
    "TrainTarget=np.array(GenerateTarget(Target,TrainingPercent,0))\n",
    "print(TrainData.shape)\n",
    "print(TrainTarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13999, 1024)\n",
      "(13999,)\n"
     ]
    }
   ],
   "source": [
    "#generating validation Data and target\n",
    "#Change ConcatFeat to SubFeat for subtraction\n",
    "ValData=GenerateData(ConcatFeat,ValidationPercent,len(TrainData))\n",
    "ValTarget=GenerateTarget(Target,ValidationPercent,len(TrainData))\n",
    "print(ValData.shape)\n",
    "print(ValTarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13999, 1024)\n",
      "(13999,)\n"
     ]
    }
   ],
   "source": [
    "#generating Test Data and target\n",
    "#Change ConcatFeat to SubFeat for subtraction\n",
    "TestData=GenerateData(ConcatFeat,TestPercent,len(TrainData)+len(ValData))\n",
    "TestTarget=GenerateTarget(Target,TestPercent,len(TrainData)+len(ValData))\n",
    "print(TestData.shape)\n",
    "print(TestTarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hidden_layer=100\n",
    "Output_layer=1\n",
    "Learning_rate=0.03\n",
    "epoch=350\n",
    "batchsize=40\n",
    "model=keras.Sequential()\n",
    "model.add(keras.layers.Dense(Hidden_layer,activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(Output_layer,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(Learning_rate),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112000 samples, validate on 13999 samples\n",
      "Epoch 1/350\n",
      "112000/112000 [==============================] - 4s 33us/step - loss: 0.6772 - acc: 0.5735 - val_loss: 0.6681 - val_acc: 0.5996\n",
      "Epoch 2/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.6379 - acc: 0.6386 - val_loss: 0.6077 - val_acc: 0.6661\n",
      "Epoch 3/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.5531 - acc: 0.7238 - val_loss: 0.5217 - val_acc: 0.7421\n",
      "Epoch 4/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.4915 - acc: 0.7631 - val_loss: 0.4821 - val_acc: 0.7643\n",
      "Epoch 5/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.4570 - acc: 0.7846 - val_loss: 0.4528 - val_acc: 0.7846\n",
      "Epoch 6/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.4291 - acc: 0.8016 - val_loss: 0.4316 - val_acc: 0.7967\n",
      "Epoch 7/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.4069 - acc: 0.8161 - val_loss: 0.4189 - val_acc: 0.8076\n",
      "Epoch 8/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3887 - acc: 0.8255 - val_loss: 0.3950 - val_acc: 0.8183\n",
      "Epoch 9/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3727 - acc: 0.8337 - val_loss: 0.3864 - val_acc: 0.8206\n",
      "Epoch 10/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3583 - acc: 0.8417 - val_loss: 0.3724 - val_acc: 0.8323\n",
      "Epoch 11/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3449 - acc: 0.8504 - val_loss: 0.3893 - val_acc: 0.8186\n",
      "Epoch 12/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3313 - acc: 0.8575 - val_loss: 0.3607 - val_acc: 0.8351\n",
      "Epoch 13/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.3186 - acc: 0.8637 - val_loss: 0.3395 - val_acc: 0.8499\n",
      "Epoch 14/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.3065 - acc: 0.8700 - val_loss: 0.3307 - val_acc: 0.8531\n",
      "Epoch 15/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2944 - acc: 0.8763 - val_loss: 0.3224 - val_acc: 0.8565\n",
      "Epoch 16/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2828 - acc: 0.8828 - val_loss: 0.3275 - val_acc: 0.8558\n",
      "Epoch 17/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2725 - acc: 0.8874 - val_loss: 0.3051 - val_acc: 0.8651\n",
      "Epoch 18/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2611 - acc: 0.8934 - val_loss: 0.2958 - val_acc: 0.8698\n",
      "Epoch 19/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2516 - acc: 0.8976 - val_loss: 0.2983 - val_acc: 0.8698\n",
      "Epoch 20/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2425 - acc: 0.9021 - val_loss: 0.2852 - val_acc: 0.8786\n",
      "Epoch 21/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2335 - acc: 0.9064 - val_loss: 0.2884 - val_acc: 0.8749\n",
      "Epoch 22/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2250 - acc: 0.9104 - val_loss: 0.2708 - val_acc: 0.8843\n",
      "Epoch 23/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2167 - acc: 0.9145 - val_loss: 0.2683 - val_acc: 0.8843\n",
      "Epoch 24/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2090 - acc: 0.9178 - val_loss: 0.2596 - val_acc: 0.8896\n",
      "Epoch 25/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.2014 - acc: 0.9217 - val_loss: 0.2544 - val_acc: 0.8893\n",
      "Epoch 26/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.1948 - acc: 0.9251 - val_loss: 0.2804 - val_acc: 0.8788\n",
      "Epoch 27/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1880 - acc: 0.9275 - val_loss: 0.2539 - val_acc: 0.8925\n",
      "Epoch 28/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.1818 - acc: 0.9308 - val_loss: 0.2421 - val_acc: 0.8960\n",
      "Epoch 29/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.1753 - acc: 0.9337 - val_loss: 0.2387 - val_acc: 0.8977\n",
      "Epoch 30/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1701 - acc: 0.9365 - val_loss: 0.2425 - val_acc: 0.8993\n",
      "Epoch 31/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1643 - acc: 0.9384 - val_loss: 0.2321 - val_acc: 0.9018\n",
      "Epoch 32/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1586 - acc: 0.9410 - val_loss: 0.2359 - val_acc: 0.9012\n",
      "Epoch 33/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.1541 - acc: 0.9429 - val_loss: 0.2339 - val_acc: 0.9020\n",
      "Epoch 34/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.1492 - acc: 0.9456 - val_loss: 0.2321 - val_acc: 0.9049\n",
      "Epoch 35/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.1445 - acc: 0.9478 - val_loss: 0.2277 - val_acc: 0.9051\n",
      "Epoch 36/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.1406 - acc: 0.9494 - val_loss: 0.2198 - val_acc: 0.9076\n",
      "Epoch 37/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.1361 - acc: 0.9514 - val_loss: 0.2195 - val_acc: 0.9093\n",
      "Epoch 38/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.1318 - acc: 0.9533 - val_loss: 0.2198 - val_acc: 0.9071\n",
      "Epoch 39/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1284 - acc: 0.9544 - val_loss: 0.2174 - val_acc: 0.9083\n",
      "Epoch 40/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1245 - acc: 0.9565 - val_loss: 0.2142 - val_acc: 0.9106\n",
      "Epoch 41/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1207 - acc: 0.9587 - val_loss: 0.2133 - val_acc: 0.9128\n",
      "Epoch 42/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1175 - acc: 0.9596 - val_loss: 0.2124 - val_acc: 0.9106\n",
      "Epoch 43/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1144 - acc: 0.9606 - val_loss: 0.2147 - val_acc: 0.9144\n",
      "Epoch 44/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1109 - acc: 0.9623 - val_loss: 0.2091 - val_acc: 0.9140\n",
      "Epoch 45/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1080 - acc: 0.9636 - val_loss: 0.2121 - val_acc: 0.9112\n",
      "Epoch 46/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1051 - acc: 0.9649 - val_loss: 0.2071 - val_acc: 0.9154\n",
      "Epoch 47/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.1024 - acc: 0.9658 - val_loss: 0.2077 - val_acc: 0.9152\n",
      "Epoch 48/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0991 - acc: 0.9680 - val_loss: 0.2220 - val_acc: 0.9071\n",
      "Epoch 49/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0966 - acc: 0.9687 - val_loss: 0.2051 - val_acc: 0.9170\n",
      "Epoch 50/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0939 - acc: 0.9705 - val_loss: 0.2127 - val_acc: 0.9121\n",
      "Epoch 51/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0916 - acc: 0.9709 - val_loss: 0.2065 - val_acc: 0.9142\n",
      "Epoch 52/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0890 - acc: 0.9724 - val_loss: 0.2038 - val_acc: 0.9179\n",
      "Epoch 53/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0865 - acc: 0.9736 - val_loss: 0.2089 - val_acc: 0.9156\n",
      "Epoch 54/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0840 - acc: 0.9744 - val_loss: 0.2043 - val_acc: 0.9174\n",
      "Epoch 55/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0819 - acc: 0.9751 - val_loss: 0.2081 - val_acc: 0.9153\n",
      "Epoch 56/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0798 - acc: 0.9758 - val_loss: 0.2057 - val_acc: 0.9167\n",
      "Epoch 57/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0775 - acc: 0.9774 - val_loss: 0.2037 - val_acc: 0.9176\n",
      "Epoch 58/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0761 - acc: 0.9777 - val_loss: 0.2075 - val_acc: 0.9178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0736 - acc: 0.9793 - val_loss: 0.2041 - val_acc: 0.9175\n",
      "Epoch 60/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0719 - acc: 0.9797 - val_loss: 0.2076 - val_acc: 0.9178\n",
      "Epoch 61/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0703 - acc: 0.9800 - val_loss: 0.2055 - val_acc: 0.9178\n",
      "Epoch 62/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0678 - acc: 0.9817 - val_loss: 0.2098 - val_acc: 0.9180\n",
      "Epoch 63/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0668 - acc: 0.9821 - val_loss: 0.2072 - val_acc: 0.9178\n",
      "Epoch 64/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0648 - acc: 0.9826 - val_loss: 0.2128 - val_acc: 0.9141\n",
      "Epoch 65/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0632 - acc: 0.9841 - val_loss: 0.2252 - val_acc: 0.9128\n",
      "Epoch 66/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0613 - acc: 0.9847 - val_loss: 0.2113 - val_acc: 0.9169\n",
      "Epoch 67/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0598 - acc: 0.9854 - val_loss: 0.2158 - val_acc: 0.9163\n",
      "Epoch 68/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0581 - acc: 0.9861 - val_loss: 0.2103 - val_acc: 0.9179\n",
      "Epoch 69/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0566 - acc: 0.9867 - val_loss: 0.2111 - val_acc: 0.9166\n",
      "Epoch 70/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0555 - acc: 0.9870 - val_loss: 0.2084 - val_acc: 0.9189\n",
      "Epoch 71/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0540 - acc: 0.9878 - val_loss: 0.2140 - val_acc: 0.9151\n",
      "Epoch 72/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0525 - acc: 0.9885 - val_loss: 0.2090 - val_acc: 0.9177\n",
      "Epoch 73/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0509 - acc: 0.9891 - val_loss: 0.2110 - val_acc: 0.9174\n",
      "Epoch 74/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0497 - acc: 0.9898 - val_loss: 0.2112 - val_acc: 0.9181\n",
      "Epoch 75/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0487 - acc: 0.9901 - val_loss: 0.2156 - val_acc: 0.9181\n",
      "Epoch 76/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0472 - acc: 0.9908 - val_loss: 0.2134 - val_acc: 0.9189\n",
      "Epoch 77/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0460 - acc: 0.9915 - val_loss: 0.2207 - val_acc: 0.9169\n",
      "Epoch 78/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0448 - acc: 0.9919 - val_loss: 0.2210 - val_acc: 0.9176\n",
      "Epoch 79/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0438 - acc: 0.9926 - val_loss: 0.2194 - val_acc: 0.9175\n",
      "Epoch 80/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0427 - acc: 0.9930 - val_loss: 0.2141 - val_acc: 0.9181\n",
      "Epoch 81/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0415 - acc: 0.9933 - val_loss: 0.2168 - val_acc: 0.9179\n",
      "Epoch 82/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0406 - acc: 0.9937 - val_loss: 0.2203 - val_acc: 0.9189\n",
      "Epoch 83/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0398 - acc: 0.9938 - val_loss: 0.2186 - val_acc: 0.9186\n",
      "Epoch 84/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0386 - acc: 0.9946 - val_loss: 0.2171 - val_acc: 0.9191\n",
      "Epoch 85/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0378 - acc: 0.9948 - val_loss: 0.2176 - val_acc: 0.9189\n",
      "Epoch 86/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0369 - acc: 0.9952 - val_loss: 0.2173 - val_acc: 0.9191\n",
      "Epoch 87/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0359 - acc: 0.9956 - val_loss: 0.2205 - val_acc: 0.9172\n",
      "Epoch 88/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0350 - acc: 0.9959 - val_loss: 0.2192 - val_acc: 0.9187\n",
      "Epoch 89/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0342 - acc: 0.9961 - val_loss: 0.2263 - val_acc: 0.9178\n",
      "Epoch 90/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0333 - acc: 0.9965 - val_loss: 0.2215 - val_acc: 0.9189\n",
      "Epoch 91/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0326 - acc: 0.9964 - val_loss: 0.2226 - val_acc: 0.9181\n",
      "Epoch 92/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0317 - acc: 0.9970 - val_loss: 0.2230 - val_acc: 0.9196\n",
      "Epoch 93/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0311 - acc: 0.9969 - val_loss: 0.2236 - val_acc: 0.9195\n",
      "Epoch 94/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0302 - acc: 0.9975 - val_loss: 0.2237 - val_acc: 0.9188\n",
      "Epoch 95/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0295 - acc: 0.9976 - val_loss: 0.2254 - val_acc: 0.9184\n",
      "Epoch 96/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0290 - acc: 0.9978 - val_loss: 0.2264 - val_acc: 0.9196\n",
      "Epoch 97/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0282 - acc: 0.9980 - val_loss: 0.2239 - val_acc: 0.9194\n",
      "Epoch 98/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0275 - acc: 0.9981 - val_loss: 0.2269 - val_acc: 0.9181\n",
      "Epoch 99/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0271 - acc: 0.9982 - val_loss: 0.2254 - val_acc: 0.9199\n",
      "Epoch 100/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0264 - acc: 0.9986 - val_loss: 0.2279 - val_acc: 0.9189\n",
      "Epoch 101/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0258 - acc: 0.9986 - val_loss: 0.2355 - val_acc: 0.9174\n",
      "Epoch 102/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0254 - acc: 0.9986 - val_loss: 0.2320 - val_acc: 0.9189\n",
      "Epoch 103/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0248 - acc: 0.9988 - val_loss: 0.2309 - val_acc: 0.9181\n",
      "Epoch 104/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0242 - acc: 0.9990 - val_loss: 0.2369 - val_acc: 0.9184\n",
      "Epoch 105/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0237 - acc: 0.9990 - val_loss: 0.2324 - val_acc: 0.9196\n",
      "Epoch 106/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0231 - acc: 0.9992 - val_loss: 0.2305 - val_acc: 0.9178\n",
      "Epoch 107/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0226 - acc: 0.9993 - val_loss: 0.2334 - val_acc: 0.9166\n",
      "Epoch 108/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0223 - acc: 0.9993 - val_loss: 0.2353 - val_acc: 0.9192\n",
      "Epoch 109/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0217 - acc: 0.9995 - val_loss: 0.2414 - val_acc: 0.9174\n",
      "Epoch 110/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0213 - acc: 0.9996 - val_loss: 0.2345 - val_acc: 0.9194\n",
      "Epoch 111/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0209 - acc: 0.9995 - val_loss: 0.2358 - val_acc: 0.9179\n",
      "Epoch 112/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0205 - acc: 0.9996 - val_loss: 0.2401 - val_acc: 0.9184\n",
      "Epoch 113/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0201 - acc: 0.9996 - val_loss: 0.2365 - val_acc: 0.9176\n",
      "Epoch 114/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0197 - acc: 0.9997 - val_loss: 0.2381 - val_acc: 0.9189\n",
      "Epoch 115/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0193 - acc: 0.9997 - val_loss: 0.2417 - val_acc: 0.9191\n",
      "Epoch 116/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0190 - acc: 0.9998 - val_loss: 0.2398 - val_acc: 0.9181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0186 - acc: 0.9998 - val_loss: 0.2395 - val_acc: 0.9187\n",
      "Epoch 118/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0183 - acc: 0.9998 - val_loss: 0.2410 - val_acc: 0.9179\n",
      "Epoch 119/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0179 - acc: 0.9998 - val_loss: 0.2420 - val_acc: 0.9191\n",
      "Epoch 120/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0177 - acc: 0.9998 - val_loss: 0.2413 - val_acc: 0.9182\n",
      "Epoch 121/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0173 - acc: 0.9999 - val_loss: 0.2423 - val_acc: 0.9185\n",
      "Epoch 122/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0169 - acc: 0.9998 - val_loss: 0.2467 - val_acc: 0.9176\n",
      "Epoch 123/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0166 - acc: 0.9999 - val_loss: 0.2438 - val_acc: 0.9179\n",
      "Epoch 124/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0163 - acc: 0.9999 - val_loss: 0.2450 - val_acc: 0.9181\n",
      "Epoch 125/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0160 - acc: 0.9999 - val_loss: 0.2542 - val_acc: 0.9169\n",
      "Epoch 126/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0158 - acc: 0.9999 - val_loss: 0.2463 - val_acc: 0.9173\n",
      "Epoch 127/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0156 - acc: 0.9999 - val_loss: 0.2519 - val_acc: 0.9181\n",
      "Epoch 128/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.2472 - val_acc: 0.9189\n",
      "Epoch 129/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.2479 - val_acc: 0.9184\n",
      "Epoch 130/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.2482 - val_acc: 0.9186\n",
      "Epoch 131/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.2510 - val_acc: 0.9179\n",
      "Epoch 132/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.2510 - val_acc: 0.9188\n",
      "Epoch 133/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.2500 - val_acc: 0.9181\n",
      "Epoch 134/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.2553 - val_acc: 0.9175\n",
      "Epoch 135/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.9185\n",
      "Epoch 136/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.2562 - val_acc: 0.9180\n",
      "Epoch 137/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.2519 - val_acc: 0.9181\n",
      "Epoch 138/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.2521 - val_acc: 0.9187\n",
      "Epoch 139/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.2521 - val_acc: 0.9181\n",
      "Epoch 140/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.2535 - val_acc: 0.9169\n",
      "Epoch 141/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.2542 - val_acc: 0.9186\n",
      "Epoch 142/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.2542 - val_acc: 0.9178\n",
      "Epoch 143/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.2548 - val_acc: 0.9171\n",
      "Epoch 144/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.2553 - val_acc: 0.9174\n",
      "Epoch 145/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.2562 - val_acc: 0.9176\n",
      "Epoch 146/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.2570 - val_acc: 0.9177\n",
      "Epoch 147/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.2592 - val_acc: 0.9175\n",
      "Epoch 148/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.2576 - val_acc: 0.9182\n",
      "Epoch 149/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.2595 - val_acc: 0.9181\n",
      "Epoch 150/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.2608 - val_acc: 0.9176\n",
      "Epoch 151/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.2607 - val_acc: 0.9168\n",
      "Epoch 152/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.2593 - val_acc: 0.9171\n",
      "Epoch 153/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.2610 - val_acc: 0.9176\n",
      "Epoch 154/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.2631 - val_acc: 0.9179\n",
      "Epoch 155/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.2614 - val_acc: 0.9186\n",
      "Epoch 156/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.2608 - val_acc: 0.9181\n",
      "Epoch 157/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.2618 - val_acc: 0.9181\n",
      "Epoch 158/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.2648 - val_acc: 0.9180\n",
      "Epoch 159/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.2625 - val_acc: 0.9179\n",
      "Epoch 160/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.2649 - val_acc: 0.9173\n",
      "Epoch 161/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.2665 - val_acc: 0.9178\n",
      "Epoch 162/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2647 - val_acc: 0.9179\n",
      "Epoch 163/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.2662 - val_acc: 0.9179\n",
      "Epoch 164/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2654 - val_acc: 0.9172\n",
      "Epoch 165/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.2661 - val_acc: 0.9177\n",
      "Epoch 166/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.2664 - val_acc: 0.9178\n",
      "Epoch 167/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.2684 - val_acc: 0.9177\n",
      "Epoch 168/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2669 - val_acc: 0.9181\n",
      "Epoch 169/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.2687 - val_acc: 0.9179\n",
      "Epoch 170/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.2721 - val_acc: 0.9171\n",
      "Epoch 171/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.2688 - val_acc: 0.9164\n",
      "Epoch 172/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.2682 - val_acc: 0.9175\n",
      "Epoch 173/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.2707 - val_acc: 0.9171\n",
      "Epoch 174/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.2711 - val_acc: 0.9168\n",
      "Epoch 175/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.2684 - val_acc: 0.9179\n",
      "Epoch 176/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.2709 - val_acc: 0.9174\n",
      "Epoch 177/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.2714 - val_acc: 0.9174\n",
      "Epoch 178/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.2744 - val_acc: 0.9164\n",
      "Epoch 179/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.2707 - val_acc: 0.9180\n",
      "Epoch 180/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.2737 - val_acc: 0.9171\n",
      "Epoch 181/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.2738 - val_acc: 0.9179\n",
      "Epoch 182/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.2725 - val_acc: 0.9174\n",
      "Epoch 183/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2729 - val_acc: 0.9173\n",
      "Epoch 184/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.2776 - val_acc: 0.9172\n",
      "Epoch 185/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.2728 - val_acc: 0.9175\n",
      "Epoch 186/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.2746 - val_acc: 0.9173\n",
      "Epoch 187/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2753 - val_acc: 0.9168\n",
      "Epoch 188/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2744 - val_acc: 0.9177\n",
      "Epoch 189/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2761 - val_acc: 0.9175\n",
      "Epoch 190/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.2759 - val_acc: 0.9174\n",
      "Epoch 191/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2785 - val_acc: 0.9163\n",
      "Epoch 192/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2781 - val_acc: 0.9176\n",
      "Epoch 193/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.2779 - val_acc: 0.9166\n",
      "Epoch 194/350\n",
      "112000/112000 [==============================] - 4s 33us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2780 - val_acc: 0.9181\n",
      "Epoch 195/350\n",
      "112000/112000 [==============================] - 4s 37us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 0.9171\n",
      "Epoch 196/350\n",
      "112000/112000 [==============================] - 4s 37us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.2785 - val_acc: 0.9170\n",
      "Epoch 197/350\n",
      "112000/112000 [==============================] - 4s 36us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.2795 - val_acc: 0.9178\n",
      "Epoch 198/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.2775 - val_acc: 0.9173\n",
      "Epoch 199/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2780 - val_acc: 0.9182\n",
      "Epoch 200/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2797 - val_acc: 0.9169\n",
      "Epoch 201/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.2800 - val_acc: 0.9173\n",
      "Epoch 202/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.2805 - val_acc: 0.9172\n",
      "Epoch 203/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2824 - val_acc: 0.9175\n",
      "Epoch 204/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2832 - val_acc: 0.9173\n",
      "Epoch 205/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2820 - val_acc: 0.9171\n",
      "Epoch 206/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2813 - val_acc: 0.9169\n",
      "Epoch 207/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2817 - val_acc: 0.9171\n",
      "Epoch 208/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2811 - val_acc: 0.9176\n",
      "Epoch 209/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2821 - val_acc: 0.9175\n",
      "Epoch 210/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2837 - val_acc: 0.9174\n",
      "Epoch 211/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2829 - val_acc: 0.9172\n",
      "Epoch 212/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2831 - val_acc: 0.9182\n",
      "Epoch 213/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2824 - val_acc: 0.9179\n",
      "Epoch 214/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2842 - val_acc: 0.9176\n",
      "Epoch 215/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2865 - val_acc: 0.9171\n",
      "Epoch 216/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.9172\n",
      "Epoch 217/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.2860 - val_acc: 0.9166\n",
      "Epoch 218/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.2864 - val_acc: 0.9165\n",
      "Epoch 219/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2852 - val_acc: 0.9178\n",
      "Epoch 220/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2855 - val_acc: 0.9181\n",
      "Epoch 221/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 0.9165\n",
      "Epoch 222/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2889 - val_acc: 0.9164\n",
      "Epoch 223/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2866 - val_acc: 0.9176\n",
      "Epoch 224/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2867 - val_acc: 0.9174\n",
      "Epoch 225/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2882 - val_acc: 0.9166\n",
      "Epoch 226/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2874 - val_acc: 0.9172\n",
      "Epoch 227/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.2893 - val_acc: 0.9163\n",
      "Epoch 228/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.2879 - val_acc: 0.9175\n",
      "Epoch 229/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2881 - val_acc: 0.9172\n",
      "Epoch 230/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2896 - val_acc: 0.9173\n",
      "Epoch 231/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2905 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2893 - val_acc: 0.9168\n",
      "Epoch 233/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.9159\n",
      "Epoch 234/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2901 - val_acc: 0.9169\n",
      "Epoch 235/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.9169\n",
      "Epoch 236/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2921 - val_acc: 0.9172\n",
      "Epoch 237/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2913 - val_acc: 0.9166\n",
      "Epoch 238/350\n",
      "112000/112000 [==============================] - 4s 35us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.9176\n",
      "Epoch 239/350\n",
      "112000/112000 [==============================] - 5s 42us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2913 - val_acc: 0.9181\n",
      "Epoch 240/350\n",
      "112000/112000 [==============================] - 5s 46us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2930 - val_acc: 0.9166\n",
      "Epoch 241/350\n",
      "112000/112000 [==============================] - 4s 33us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.2930 - val_acc: 0.9169\n",
      "Epoch 242/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.2924 - val_acc: 0.9170\n",
      "Epoch 243/350\n",
      "112000/112000 [==============================] - 4s 35us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.2928 - val_acc: 0.9176\n",
      "Epoch 244/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.2925 - val_acc: 0.9179\n",
      "Epoch 245/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.9166\n",
      "Epoch 246/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.2941 - val_acc: 0.9170\n",
      "Epoch 247/350\n",
      "112000/112000 [==============================] - 4s 32us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2937 - val_acc: 0.9173\n",
      "Epoch 248/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.9175\n",
      "Epoch 249/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2940 - val_acc: 0.9180\n",
      "Epoch 250/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2944 - val_acc: 0.9174\n",
      "Epoch 251/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2943 - val_acc: 0.9179\n",
      "Epoch 252/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2960 - val_acc: 0.9162\n",
      "Epoch 253/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 0.9167\n",
      "Epoch 254/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.2953 - val_acc: 0.9165\n",
      "Epoch 255/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.2958 - val_acc: 0.9169\n",
      "Epoch 256/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.2962 - val_acc: 0.9166\n",
      "Epoch 257/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.2966 - val_acc: 0.9166\n",
      "Epoch 258/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2973 - val_acc: 0.9166\n",
      "Epoch 259/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2967 - val_acc: 0.9164\n",
      "Epoch 260/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.2984 - val_acc: 0.9164\n",
      "Epoch 261/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 0.9169\n",
      "Epoch 262/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2993 - val_acc: 0.9159\n",
      "Epoch 263/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.9181\n",
      "Epoch 264/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2973 - val_acc: 0.9171\n",
      "Epoch 265/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2987 - val_acc: 0.9171\n",
      "Epoch 266/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2983 - val_acc: 0.9166\n",
      "Epoch 267/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2991 - val_acc: 0.9164\n",
      "Epoch 268/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2989 - val_acc: 0.9172\n",
      "Epoch 269/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 0.9165\n",
      "Epoch 270/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2991 - val_acc: 0.9173\n",
      "Epoch 271/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 0.9169\n",
      "Epoch 272/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 0.9174\n",
      "Epoch 273/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 0.9166\n",
      "Epoch 274/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.3005 - val_acc: 0.9169\n",
      "Epoch 275/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.2997 - val_acc: 0.9179\n",
      "Epoch 276/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.3012 - val_acc: 0.9163\n",
      "Epoch 277/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.3008 - val_acc: 0.9160\n",
      "Epoch 278/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.3013 - val_acc: 0.9166\n",
      "Epoch 279/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3023 - val_acc: 0.9161\n",
      "Epoch 280/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3014 - val_acc: 0.9167\n",
      "Epoch 281/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.9173\n",
      "Epoch 282/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 0.9164\n",
      "Epoch 283/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3013 - val_acc: 0.9179\n",
      "Epoch 284/350\n",
      "112000/112000 [==============================] - 4s 31us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3028 - val_acc: 0.9163\n",
      "Epoch 285/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3037 - val_acc: 0.9160\n",
      "Epoch 286/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3029 - val_acc: 0.9170\n",
      "Epoch 287/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3037 - val_acc: 0.9165\n",
      "Epoch 288/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3058 - val_acc: 0.9159\n",
      "Epoch 289/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3035 - val_acc: 0.9167\n",
      "Epoch 290/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3053 - val_acc: 0.9164\n",
      "Epoch 291/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3047 - val_acc: 0.9161\n",
      "Epoch 292/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3043 - val_acc: 0.9165\n",
      "Epoch 293/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3047 - val_acc: 0.9161\n",
      "Epoch 294/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.9161\n",
      "Epoch 295/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3051 - val_acc: 0.9166\n",
      "Epoch 296/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3049 - val_acc: 0.9171\n",
      "Epoch 297/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3051 - val_acc: 0.9166\n",
      "Epoch 298/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3053 - val_acc: 0.9168\n",
      "Epoch 299/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 0.9161\n",
      "Epoch 300/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3078 - val_acc: 0.9166\n",
      "Epoch 301/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3078 - val_acc: 0.9164\n",
      "Epoch 302/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.9165\n",
      "Epoch 303/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3087 - val_acc: 0.9158\n",
      "Epoch 304/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.9169\n",
      "Epoch 305/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3077 - val_acc: 0.9166\n",
      "Epoch 306/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3078 - val_acc: 0.9164\n",
      "Epoch 307/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3077 - val_acc: 0.9171\n",
      "Epoch 308/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3073 - val_acc: 0.9170\n",
      "Epoch 309/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3084 - val_acc: 0.9169\n",
      "Epoch 310/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9173\n",
      "Epoch 311/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3084 - val_acc: 0.9166\n",
      "Epoch 312/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 0.9164\n",
      "Epoch 313/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3090 - val_acc: 0.9164\n",
      "Epoch 314/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3081 - val_acc: 0.9179\n",
      "Epoch 315/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3092 - val_acc: 0.9161\n",
      "Epoch 316/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3087 - val_acc: 0.9168\n",
      "Epoch 317/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3090 - val_acc: 0.9172\n",
      "Epoch 318/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3092 - val_acc: 0.9164\n",
      "Epoch 319/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3091 - val_acc: 0.9166\n",
      "Epoch 320/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3105 - val_acc: 0.9157\n",
      "Epoch 321/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3103 - val_acc: 0.9168\n",
      "Epoch 322/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3097 - val_acc: 0.9166\n",
      "Epoch 323/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3109 - val_acc: 0.9161\n",
      "Epoch 324/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3109 - val_acc: 0.9164\n",
      "Epoch 325/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3112 - val_acc: 0.9170\n",
      "Epoch 326/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3121 - val_acc: 0.9159\n",
      "Epoch 327/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3111 - val_acc: 0.9164\n",
      "Epoch 328/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3117 - val_acc: 0.9162\n",
      "Epoch 329/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3113 - val_acc: 0.9165\n",
      "Epoch 330/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 0.9164\n",
      "Epoch 331/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3118 - val_acc: 0.9170\n",
      "Epoch 332/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3116 - val_acc: 0.9174\n",
      "Epoch 333/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3121 - val_acc: 0.9169\n",
      "Epoch 334/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3124 - val_acc: 0.9167\n",
      "Epoch 335/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.3126 - val_acc: 0.9173\n",
      "Epoch 336/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3124 - val_acc: 0.9171\n",
      "Epoch 337/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9164\n",
      "Epoch 338/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3132 - val_acc: 0.9170\n",
      "Epoch 339/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3129 - val_acc: 0.9167\n",
      "Epoch 340/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3152 - val_acc: 0.9163\n",
      "Epoch 341/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9165\n",
      "Epoch 342/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3140 - val_acc: 0.9171\n",
      "Epoch 343/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.3151 - val_acc: 0.9164\n",
      "Epoch 344/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 0.9168\n",
      "Epoch 345/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3151 - val_acc: 0.9169\n",
      "Epoch 346/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3148 - val_acc: 0.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 0.9165\n",
      "Epoch 348/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3151 - val_acc: 0.9164\n",
      "Epoch 349/350\n",
      "112000/112000 [==============================] - 3s 31us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3153 - val_acc: 0.9165\n",
      "Epoch 350/350\n",
      "112000/112000 [==============================] - 3s 30us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3152 - val_acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(TrainData,TrainTarget,epochs=epoch,batch_size=batchsize,validation_data=(ValData,ValTarget))\n",
    "predictedTestTarget=model.predict(TestData)\n",
    "#print(history)\n",
    "#print(predictedTestTarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8XHWd//HXJ2lTWloubUMXStu0toqsbLkEFgQEUaCwLl1+ghYDy9WqWGVVfvuDrbIsPqqgrsrKtYssiJGbl7WuRQSRiyxggwLSYmkovYRW2qZ3ek36+f3xmWkm08lk2mYycybv5+ORR+acOTnz6Wnynu98z/d8j7k7IiJSWapKXYCIiPQ8hbuISAVSuIuIVCCFu4hIBVK4i4hUIIW7iEgFUriLiFQghbuISAVSuIuIVKB+hWxkZpOAm4Fq4C53vzHr+e8AH0wtDgIOcvcD8u1z+PDhXldXt9sFi4j0ZS+++OIqd6/tbrtuw93MqoFbgdOBFmCOmc1y93npbdz9Cxnbfw44qrv91tXV0dTU1N1mIiKSwcwWF7JdId0yxwHN7r7Q3bcBDwCT82x/AXB/IS8uIiLFUUi4jwSWZiy3pNbtwszGAGOBJ/a+NBER2VOFhLvlWNfVVJJTgB+7e3vOHZlNNbMmM2tauXJloTWKiMhuKiTcW4BRGcuHAsu62HYKebpk3H2mu9e7e31tbbfnA0REZA8VEu5zgAlmNtbMaogAn5W9kZm9BzgQeK5nSxQRkd3Vbbi7exswDXgUeA14yN3nmtkNZnZOxqYXAA94Ee/+0dgIdXVQVRXfGxuL9UoiIslmpboTU319ve/OUMjGRpg6FTZt6lg3aBDMnAkNDUUoUESkDJnZi+5e3912iblCdfr0zsEOsTx9emnqEREpZ4kJ9yVLdm+9iEhflphwHz1699aLiPRliQn3GTOij/1y7mIB4+nPNgYNivUiItJZQROHlYP0SdMFn9vE+DVv8N5DN/LPNw7VyVQRkRwSE+6QCvitg+FyePl3G2DM0FKXJCJSlhLTLbPTkCHxfcOG0tYhIlLGkhfugwfH940bS1uHiEgZS164q+UuItKt5Ia7Wu4iIl1KXLj//DfRLXPx/9mg+WVERLqQqHBvbISrvhwt98FsYPHimG9GAS8i0lmiwn36dFixOVrug4luGc0vIyKyq0SNc1+yBJyBtFPFEDZ0Wi8iIh0S1XKPeWSMDQzpFO6aX0ZEpLNEhXt6fpmNDN7ZLaP5ZUREdpWobpn0PDJbLhnCkLYNjBkTwa75ZUREOktUuEMqyL87mPG1G/nY7FJXIyJSnhLVLbPTkCG6QlVEJI9khvvgwQp3EZE8Cgp3M5tkZvPNrNnMrulim4+Z2Twzm2tmP+rZMrMMGaLpB0RE8ui2z93MqoFbgdOBFmCOmc1y93kZ20wArgVOdPc1ZnZQsQoG1C0jItKNQlruxwHN7r7Q3bcBDwCTs7b5JHCru68BcPcVPVtmloEDYfPmor6EiEiSFRLuI4GlGcstqXWZ3g2828yeNbPnzWxSTxWYk8JdRCSvQoZCWo51nmM/E4BTgUOBZ8zsfe6+ttOOzKYCUwFG781lpQMHQltbfPVL3GhOEZGiK6Tl3gKMylg+FFiWY5ufu/t2d38TmE+EfSfuPtPd6929vra2dk9rhn32ie9qvYuI5FRIuM8BJpjZWDOrAaYAs7K2+W/ggwBmNpzoplnYk4V2MnBgfFe4i4jk1G24u3sbMA14FHgNeMjd55rZDWZ2TmqzR4FWM5sH/Bb4v+7eWqyid4b7li1FewkRkSQrqMPa3WcDs7PWXZfx2IEvpr6KTy13EZG8knmFqsJdRCQvhbuISAVSuIuIVCCFu4hIBVK4i4hUoGSGe/oiJg2FFBHJKZHh/tNHouU+9aLN1NVBY2Np6xERKTeJC/fGRrjqmgj3fdjM4sUwdaoCXkQkU+LCffp0aN0c4T6Q6HPftCnWi4hISNyUikuWAESfezrcO9aLiAgksOU+ejQ4VWxhQKdw35sZhEVEKk3iwn3GDBg0CDYzcGe4DxoU60VEJCSuW6ahIb5vu3ggA9u3MGZMBHt6vYiIJDDcIRXkX9mHT564mU/eV+pqRETKT+K6ZXbSfVRFRLqkcBcRqUAKdxGRCqRwFxGpQMkN9wMOgLVrS12FiEhZSm64DxsGrcW7B7eISJIVFO5mNsnM5ptZs5ldk+P5S8xspZm9lPq6oudLzZIO9x07iv5SIiJJ0+04dzOrBm4FTgdagDlmNsvd52Vt+qC7TytCjbkNHx7Bvm4dHHhgr72siEgSFNJyPw5odveF7r4NeACYXNyyCjBsWHxX14yIyC4KCfeRwNKM5ZbUumwfNbNXzOzHZjaqR6rLJx3uq1YV/aVERJKmkHC3HOs8a/kXQJ27/w3wOHBvzh2ZTTWzJjNrWrly5e5VmuVXTcMB+MgJrbobk4hIlkLCvQXIbIkfCizL3MDdW919a2rxP4Fjcu3I3We6e72719fW1u5JvUAE+dVfj5b7UFp1NyYRkSyFhPscYIKZjTWzGmAKMCtzAzM7OGPxHOC1nitxV9OnQ8uWCPdhRJ+77sYkItKh29Ey7t5mZtOAR4Fq4G53n2tmNwBN7j4L+LyZnQO0AauBS4pYM0uWgLM/bVQznFWd1ouISIFT/rr7bGB21rrrMh5fC1zbs6V1bfRoWLzYaGXYzpZ7er2IiCT0CtX03ZhWMZxa4sSs7sYkItIhuTfrANZccTAHb1muuzGJiGRJZLhDKsgfPQSefppFi0pdjYhIeUlkt8xOhxwCy5aBZw+7FxHp25If7tu3awoCEZEsiQ73p984BICJtct0laqISIbEhntjI1x/Z4T7wSzTVaoiIhkSG+7Tp8PCrRHuh6RmQ9BVqiIiIbGjZZYsgRr+CoCRvNVpvYhIX5fYlvvo0bCVffgLIxjD4k7rRUT6usSGe/oq1WbGM55mQFepioikJbZbJn016l8+NZ7j33lcV6mKiGRIbLhDKsjfHA9fuZdF8zZF011ERJLbLZP2u7+MB+CIfRdqrLuISEqiw72xEa65K8L9XTRrrLuISEqiw336dHh563top4ojeQnQWHcREUh4n3vckWkIf+IITuTZTutFRPqyRLfc02Paf8dJnMBzVNPWab2ISF+V6HBPj3X/HScxmHeYyMsa6y4iQsLDvaEBZs6ExftPBOA9zGfgwBIXJSJSBhId7mkLto0BoI5FtLZqxIyISEHhbmaTzGy+mTWb2TV5tjvPzNzM6nuuxPymT4fWzYN4m4OoYxGgETMiIt2Gu5lVA7cCZwGHAxeY2eE5thsCfB54oaeLzCc9MmYRdTvDPXO9iEhfVEjL/Tig2d0Xuvs24AFgco7tvgp8A9jSg/V1Kz0yJjvcNWJGRPqyQsJ9JLA0Y7kltW4nMzsKGOXu/5NvR2Y21cyazKxp5cqVu11sLukRM4sZwxgWY+zQiBkR6fMKCXfLsc53PmlWBXwH+FJ3O3L3me5e7+71tbW1hVeZR0MDXHwxLLE6BrCNUVXLuPhizQ4pIn1bIeHeAozKWD4UUve1C0OA9wFPmtki4HhgVm+dVG1shHvvhf/14wE4Y8cj3H/PVu6/Z2tvvLyISFkyd8+/gVk/4HXgQ8BbwBzgE+4+t4vtnwSudvemfPutr6/3pqa8mxSkrg4WLwZw5vMeljKKw/gz/aqdEW3LuvlpEZFkMbMX3b3bxnO3LXd3bwOmAY8CrwEPuftcM7vBzM7Z+1L3TseoGONBPs4H+S0jWcaI9uWlLEtEpKQKmjjM3WcDs7PWXdfFtqfufVmFGz063XKHZziZKjI+ibiD5TplICJS2RJ/heqMGdC/fzyew7Gdnnt45poSVCQiUnqJD/eGBthvv3i8lgOZz7t3PnfXv71VoqpEREor8eEOsHp1x+On+cDOx1XLFe4i0jdVRLhnXo36Jf6dk3kagCOGKtxFpG+qiHBPX6UKsIH9+D3HAXDyuzQUUkT6pooI9/RVqumBMdsYwCqG8fYf39LUvyLSJ1VEuAPMnh0jH9MWUcehbYs09a+I9EkVE+7ZU/w2M57xNGvqXxHpkyom3LOn+G1mPHUs4qADt5emIBGREqqYcM+8mAki3PvRztANi9XvLiJ9TsWEe+bFTBDhDjB6e7P63UWkz6mYcIfOFzOlw30CC9TvLiJ9TkWFe2a/+9uM4C+M4BSeYujQ0tUkIlIKFRXunfvdjYc5n4/wP7B+vfrdRaRPqahwz+53v58L2IetnL39v/nyv+woXWEiIr2sosIdOve7P8cJLGIMP+Bimpf0h9bW0hUmItKLKi7cO493N37CRwGoZgePfTfnnQFFRCpOxYX7jBmdb750B59mM/sA8Kvb3yxRVSIivaug2+wlSUMDXHhhx3IzE9ifdWxmIPu3LixdYSJS+Z55BjZuhHHj4H//F/bdF1auhNpa2Lw5+o3d4UMfgokTi1pKxYU7wJgxHfdVBdhODUsZxWE1CncR6cLGjTBgAGzZAq+9BmvWwIsvwqhRsGkTrF8PCxbAe94D77wDixbB1q0R2K++CkOGxM8V4vbbyyPczWwScDNQDdzl7jdmPf9p4LNAO7ARmOru83q41oLNmAGXXgrbM6aVWcg4Dt2+kMbGaN2LSIVYvhxeeilC9xe/gA9/GFatgvb2aDkvWgQ1NbHdjh0RwIMHx2yDgwdHOLe2wlNPQXV1bLt5c+7X2m+/CHmAgw+O/e+zD5xySgT+uefC+98PK1bAMcdEDX/1V1FPTQ2MGAFVVfEzRWaeOU9urg3MqoHXgdOBFmAOcEFmeJvZfu6+PvX4HOBKd5+Ub7/19fXe1NS0l+V3bfjwzoNj/pMr+Dt+yQljlrNoUdFeVkT2xtatcdLszTfhW9+CD3wgWstjx0ar+gc/iD/uiRPh7bfjI/rs2dGyLlRNDbS1RYt861YYOjTWnXlmBO/GjfEGceCBcNhhsHQpDBsWPzt6NKxbF+HcCwGdi5m96O713W1XSMv9OKDZ3RemdvwAMBnYGe7pYE/ZF8j/jtELModEAszjcK7g+/jixcCYktQk0uesWhWt3X794vGGDdFy3rQJHnssWr/Ll0f3x2GHwaxZ0drdsiV+/q67Ou/v4IOjhdzYGOE6Zky0li+7DH75S/jrv4Y//AFOOy3eBNavj/Bub4+fra6OUH/nHdh//8L+DbW1nZcPOGDvj0svKCTcRwJLM5ZbgL/N3sjMPgt8EagBTuuR6vbC6NGd+91/wd/zbb7EzVzFI1/9Mmd9pds3PhHJtnp1tGTffjtC+qWXYPx4OPlkePjh6Attbo5LxTdvhqefhkMOiRbx0qWd9zVkSLSS+/ePE5A/+1mMhhgyJAL5vPPipOS558bH8E2bouulf//4uSFDOg+NOy0VO5dd1v2/o9BgT7BCumXOB8509ytSyxcBx7n757rY/hOp7S/O8dxUYCrA6NGjj1mcmb49rLERLrqo892ZWhjJSJbRWjWcYe0ri/baIomyfn2E9IAB0RresCHC8ze/iVbwqFHRb/3GG9DS0vlnq6qiHxtg4MAI3ve+N9ZVV8Pxx8Prr8dNjo85Jtafckq8zhFHxJvBgAHRsm9vj5+RvArtlikk3E8Arnf3M1PL1wK4+9e72L4KWOPued8ai93nHrV0Xj6bX/JLPhIL69Z1nqtApFIsWhQnCv/4x2jdPvZYBPORR0Z3x+23w9q1cOihEeKrV8f3bGbRVbJ+ffR5jx0L73tfhH1bG5xwQoT2009HK+qcc+IEoxRVT/a5zwEmmNlY4C1gCvCJrBeb4O4LUot/ByygDGQPiZzN33E6v+YxzohfxNtvj1aGSDnasSMC1izC0yz6imfMiOcuvRR+/vMI5qeeghNPhCeegBde2HVfQ4fCvffG4xNOiN//JUuigTNoEPzDP0Qruq0t3hAGD4aRI3e9xVkuU6b07L9bekS34e7ubWY2DXiUGAp5t7vPNbMbgCZ3nwVMM7MPA9uBNcAuXTKlcPbZkd+ZXkifLnjqKfjmN+Huu3u/MOnb3GHbtuiOyFy3YEG0pvv3h7lz4WMfi26Ptrbor37/+2P9ylSX4k03dfz8mDHRgh43LkaZtLbGycXqajjppAjqVatif0OH7vqxVipOQePc3X02MDtr3XUZj6/q4bp6xOzZu67bwH78aPBUPrFxZuf+ww0b4iNr9plxkZ6wfXt8jHziiWh5L1sGZ5wRYT5gAMyZA88/H9tWV0fYjxgRXSFDh8a2zz8f/dVf+EK8OSxYAB/8YPRfjxwZfdb98vxJ63e7T+m2z71YeqPPvaqq8wnVTP6PF0df5LJlseKKK6KP8sUXi1qTJNyzz8aQunHjOtbt2AHf/nYE9IgR0dJeswbmz49zO+edB/fcExe2QJxkPPZYeOSR6Pt+551oeV92WYy73rQpfnm/8IWO8dUiKT3Z555Y2cMh08zgj9v+mqOW/yD+CA88MD7uvvZaR9+m9G1tbXDffXFhy9y50VVy0EEdLeVLLonwdocnn4yW+ZgxEdb9+8dY6JEjYz/f+EZcjHPTTTBhQvR5V2XM2affOSmCig73GTN2HQ4JsXzzE+/jHoA//Sn+8JYu7ZjYR62lvmHFCrj8cpg6NVrSjY1xqfjQoXDzzfDoozG8L/NS9EGDogvve9+LoXyrVsGnPgXHHRdjtLND2j1a4/muZlSwSxFUdLcMdP13U8sKVgysiwssfve7aGXt2BFdM0ceWfS6pJesWxcBvX17BPPatXDLLRHszzwDL78cfd7779/RbQLR8v77v48Lc66/HurqYN68GGWyfHlcuHPYYaX6V0kfpm6ZlOzhkGmr7CAe++IjnD7j1Bg1k74QY+lShXsSrVsXY6z79YNf/Qoefzze2b/73eh2W7UqhvilJ33q3z8C/b77Yttly+DGG+MS9nXr4Gtfi/lG7rorvoskTMWHe76umU/+8BQWTZwYJ8PSsq/Ak/LS3h791U8+2TH075VX4v9w/Pjoz25sjL5uiP/81avhXe+KESZjx8al8ocfHgE/aFDnGwAcfXTn11OwS0JVfLhn37wj0+LFwKenxEfztOz5L6R3bNkSLe3Msd+Zli6FBx+Ef/3XGD74+uudn7/wwpjT5Be/gI9/PC7w2bo1LnYQ6YMqPtyh664ZM3h46Kc4n2tjxfDhCveetmYN3HYbXH117uB+8EH4zneiO+WNNyKg29vhoYfiZPeyZfDcc/DWW7H98cfHG8F3vxsnQl94Ia6mrNdEcCKZ+kS45+ua+b9fO5DzH3gAfvrTuKpvrm6i3aPuvBO+/OUYH24Wx3nbtmiBv/VWhHim2tp4vr09+shra6Mb5YQTYlTTxImdz5Kfemqv/nNEkqLiR8uk5RtttvMQ3HBDjIxobY2TcLLnnn8+LjSYPBky/59PPjla44sWxTFesyb6zVtaogX/5z9HP/jkyTE8UcMERTrRaJks+bpmdt5679RTI+mfeSZC6IADFC57Ys6cmM9k2LAYXjh2bIT3WWd13JThP/4DJk2KcN+yJQL/2GNLXblIxegzLfdc87unDRsWI+XYujVak+PGRffMrFkx1lmiP/zKK+H7349ZCN9+O662nDw5JrR697tj2ODChTGm/MADowV+xBEwc2ZcCDRqVKn/FSKJ12PzuRdLb4c75G+E//CHqdb7jTfCtakTrBdcAF//ejT7+4rt22OI4J//HFPCnnFGrL/oojhIl1wSV26mW+RLl8abIsBRR8WVmjU18Uagi3xEepzCPYe6utxdMxD5vWgR0bT/8Y/h1ltjWmCIYXXf/GblT0vw/PPwoQ9Fl8m3vhXDDZ97Lo7DNddEaG/ZEiNbxo+PCa6+//0YM756dUwxq24skaJSuOfQ2Nj1mHfI6rL5r/+KWfqOOioukhk/Pm45NmhQ0essmYYG+NGPOpYHDowrd7dujftY/su/RNfLpZfGxT5VVbotmkgvU7h3YfjwGAyTzSyuRG9oSK3Yvj0uYz/77LiX5Jlnwj//c+cbJBTDjh3xLtOToTlvXrS4M6++bG+PKzz79YvzC++8A9Onw/nnx8eYNWvg85+P8en19XDddWqVi5QBhXsX8p1Y3dk1k8uUKdHXvHx5/hn+IAJ6y5Y9a+X/0z/FaJNnn939n83FPW4luGxZ3HVqxQp480349a/jE0mmujr47W/ju4iUJYV7HgWNec/2+ONw+unw2c/GaJqLLur6zjZ33hldGC0t0bWxO8aNi/BdsyaGYnZl7droHnnzTbjqqnj8ve/FJ44rr4w3oCefjH6o9NWdaQMGxMnOL30JXn01ulfOOitufjx06O7VKyK9qtBwx91L8nXMMcd4qVRXu0eMd/4yy/ND7e3uJ57YsfGoUe4LFuTe9qMfjW2amnavsKVLO/Z/yy3ua9fm3u6NN9y/8pXYbuLE+J5+TXA/5BD3ww93HziwY/lnP3N/8EH3t96Kf4uIJBJx7+puM7aq2/SvQO3tude7R7dNTlVVcXHT6tUxqmTTprhQ5+qr4ZhjYNq0GDq4Ywf8/vfxM/ffH/PDF+qZZzoeT5sWY8jdo5umoSFuEtHcHC3sr3415ll5/vlobf/kJ/CRj8StAydOjDHo554b487nzo2723/sYzF/fVWf/G8X6VsKeQcAJgHzgWbgmhzPfxGYB7wC/AYY090+S9lyHzMmd8sd4rmCvPSS+5lnuvfv33kHl17aefmgg9w3bXKfN899+3b3trZd97V2rftNN7kfdZT78OGdf76mZtflAw5wP/VU98cfj59/9ln3G25w37ath46QiJQrCmy5d9vnbmbVwOvA6UALMAe4wN3nZWzzQeAFd99kZp8BTnX3j+fbbyn73HdrSGR31q+PkSZHHx2t+fTNIDKddVbcDPnII2Ps+Ne+Fq3pzZtjed26uB8nxPjyk06K+chvuSVmRpw7N8aQNzfHjbwvugj+9m93698sIpWhx06omtkJwPXufmZq+VoAd/96F9sfBdzi7ifm228pwx12Y0hkoTZtisvyP//5OFn55JNx8c9pp8ETT3RsV1sLK1d2LNfUxCyIl10G++0XwZ95Enb79hhfX18f3TL5TrKKSMXryYnDRgKZk5y3APmajZcDjxSw35K6+eaupwG+6qo9CPdBg+LrgQdiee3aGPEyYkTMNnnGGdFK/8Qn4qPD8uUx5HDChBiqOHx47v3279/RSlewi0iBCmm5nw+c6e5XpJYvAo5z98/l2PZCYBpwirtvzfH8VGAqwOjRo49Z3NVcAL2koLlmRETKSKEt90KGTbQAmdP5HQosy/GCHwamA+fkCnYAd5/p7vXuXl/b1RjxXpRvPrCrruq9OkREeloh4T4HmGBmY82sBpgCzMrcINXPficR7Ct6vszimDGj6+daW+NaIBGRJOo23N29jehqeRR4DXjI3eea2Q1mdk5qs28Cg4GHzewlM5vVxe7KSkND/oke77gjz7h3EZEy1ienH8jU3bDInTfyEBEpAz3Z517Rumu9t7aq9S4iydPnwx1iWGS+kTM6uSoiSaNwJ1rvn/5018+r9S4iSaNwT7nttvzdM9On914tIiJ7S+Ge4eabu36uxNdbiYjsFoV7hoaG/LPhaty7iCSFwj3Ljh1dP6dx7yKSFAr3LPmmJEhPKiYiUu4U7llmzMg/LFLTEohIEijcs3Q3LBLg9tsV8CJS3hTuOdx2G3zmM/m3Uf+7iJQzhXsXuhv3rv53ESlnCvc8upuWQP3vIlKuFO55FNr/ru4ZESk3CvduFNL//qlP9U4tIiKFUrgXoLv+93feUfeMiJQXhXuB8s07AxoeKSLlReFeoIYGGDw4/zYKeBEpFwr33XDHHd1vo4AXkXKgcN8NDQ3dn1wFBbyIlJ7CfTcVMnoGFPAiUloFhbuZTTKz+WbWbGbX5Hj+A2b2BzNrM7Pzer7M8rI7Aa8x8CJSCt2Gu5lVA7cCZwGHAxeY2eFZmy0BLgF+1NMFlqtCA/6iixTwItL7Cmm5Hwc0u/tCd98GPABMztzA3Re5+ytAnltdVJ5CAt4dLrxQXTQi0rsKCfeRwNKM5ZbUut1mZlPNrMnMmlauXLknuyg76oMXkXJUSLjnmjrL9+TF3H2mu9e7e31tbe2e7KIs3XZb92PgIQJ+yBB104hI8RUS7i3AqIzlQ4FlxSknuQoZAw+wcaO6aUSk+AoJ9znABDMba2Y1wBRgVnHLSp5Cx8CnqZtGRIqp23B39zZgGvAo8BrwkLvPNbMbzOwcADM71sxagPOBO81sbjGLLle33QY//CHsu29h26ubRkSKxdz3qPt8r9XX13tTU1NJXrs3XHllhHehPvOZeHMQEcnHzF509/ruttMVqkVS6CiaNLXiRaQnKdyLKN1Nk+9WfZl0slVEeorCvcgaGuC++6C6uvCfUSteRPaWwr0XNDTAvfcWfqIVOlrxCnkR2RMK917S0BCBvTujaaAj5Kuq1F0jIoVTuPeydMjvzslWiDlqbr8dBg5US15EuqdwL5HdHU2TtmVLtOTNYPhwBb2I5KZwL6HdvegpW2ur+uVFJDeFe4ntaV98pnS/vFrzIpKmcC8TPRHy0NGaV9CL9G0K9zKTGfI1NXu3LwW9SN+lcC9TDQ2wdevet+TTMoNeYS9S+RTuZS6zJT9sWM/tV2EvUtkU7gnR0ACrVsV49z0ZQtmd7LA30ygckSRTuCdQeghlT7bkc8kchaPQF0kWhXtCZbbk3Xsn7NO6Cv3ML3XziJSWwr1CZHfbFDrNcLHk6ubRG4FI71G4V6DbboMdO3q/Rb+ndueNQG8OIoVRuFe47O6bcmjV97S9fXPQG4pUIoV7H5PZqk9Ky74cleINpdy/9IZXXgoKdzObZGbzzazZzK7J8fwAM3sw9fwLZlbX04VKcWS37BX4sqf0hldeb4bdhruZVQO3AmcBhwMXmNnhWZtdDqxx9/HAd4CberpQ6T25Al+hL9KzWlvhssuKF/CFtNyPA5rdfaG7bwMeACZnbTMZuDf1+MfAh8zMeq5MKQddhb7eAET2zLZtMH16cfZdSLiPBJZmLLek1uXcxt3bgHWA/sT7oELeAPQmINJhyZLi7LeQcM/VAvc92AYzm2pmTWbWtHLlykLqkwpV6JuA3iCk0o2vaJvnAAAFWUlEQVQeXZz9FhLuLcCojOVDgWVdbWNm/YD9gdXZO3L3me5e7+71tbW1e1axSEpPvEHoDUVKqaYGZswozr4LCfc5wAQzG2tmNcAUYFbWNrOAi1OPzwOecPddWu4iSVeKN5Ry/+qpaan7mmHD4O6743eqGPp1t4G7t5nZNOBRoBq4293nmtkNQJO7zwK+D9xnZs1Ei31KccoVkXLT0FC8gJI91224A7j7bGB21rrrMh5vAc7v2dJERGRP6QpVEZEKpHAXEalACncRkQqkcBcRqUBWqhGLZrYSWLwHPzocWNXD5RST6i2eJNUKyao3SbVC36p3jLt3e6FQycJ9T5lZk7vXl7qOQqne4klSrZCsepNUK6jeXNQtIyJSgRTuIiIVKInhPrPUBewm1Vs8SaoVklVvkmoF1buLxPW5i4hI95LYchcRkW4kKty7u5drOTCzRWb2JzN7ycyaUuuGmtljZrYg9f3AEtV2t5mtMLNXM9blrM3Cf6SO9StmdnSZ1Hu9mb2VOr4vmdnZGc9dm6p3vpmd2cu1jjKz35rZa2Y218yuSq0vy+Obp96yO75mto+Z/d7MXk7V+m+p9WNT92xekLqHc01qfUnv6Zyn3nvM7M2MY3tkan1xfhfcPRFfxIyUbwDjgBrgZeDwUteVo85FwPCsdd8Arkk9vga4qUS1fQA4Gni1u9qAs4FHiBuxHA+8UCb1Xg9cnWPbw1O/EwOAsanfleperPVg4OjU4yHA66mayvL45qm37I5v6hgNTj3uD7yQOmYPAVNS6+8APpN6fCVwR+rxFODBXj62XdV7D3Beju2L8ruQpJZ7IfdyLVeZ95i9F/iHUhTh7k+z601UuqptMvADD88DB5jZwb1Taeii3q5MBh5w963u/ibQTPzO9Ap3X+7uf0g93gC8Rtx+siyPb556u1Ky45s6RhtTi/1TXw6cRtyzGXY9tiW7p3OeertSlN+FJIV7IfdyLQcO/NrMXjSzqal1I9x9OcQfFXBQyarbVVe1lfPxnpb6+Hp3RhdX2dSb6gY4imixlf3xzaoXyvD4mlm1mb0ErAAeIz45rPW4Z3N2PSW/p3N2ve6ePrYzUsf2O2Y2ILvelB45tkkK94Lu01oGTnT3o4GzgM+a2QdKXdAeKtfjfTvwLuBIYDnw76n1ZVGvmQ0GfgL8k7uvz7dpjnXlUG9ZHl93b3f3I4nbfB4HvDdPPSU/ttn1mtn7gGuBw4BjgaHA/0ttXpR6kxTuhdzLteTcfVnq+wrgZ8Qv4tvpj1mp7ytKV+EuuqqtLI+3u7+d+sPZAfwnHV0DJa/XzPoTQdno7j9NrS7b45ur3nI+vqn61gJPEn3TB1jcszm7noLu6dwbMuqdlOoKc3ffCvwXRT62SQr3Qu7lWlJmtq+ZDUk/Bs4AXqXzPWYvBn5emgpz6qq2WcA/ps7kHw+sS3cvlFJWX+S5xPGFqHdKaqTEWGAC8PterMuI202+5u7fzniqLI9vV/WW4/E1s1ozOyD1eCDwYeIcwW+JezbDrse2ZPd07qLeP2e8yRtxfiDz2Pb870JvnUHuiS/irPLrRH/b9FLXk6O+ccSIgpeBuekaif6+3wALUt+Hlqi++4mP2tuJ1sLlXdVGfFS8NXWs/wTUl0m996XqeSX1R3FwxvbTU/XOB87q5VpPIj5KvwK8lPo6u1yPb556y+74An8D/DFV06vAdan144g3mGbgYWBAav0+qeXm1PPjevnYdlXvE6lj+yrwQzpG1BTld0FXqIqIVKAkdcuIiEiBFO4iIhVI4S4iUoEU7iIiFUjhLiJSgRTuIiIVSOEuIlKBFO4iIhXo/wNyXbM9ySu7vAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(acc)+1)\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss,'r',label='Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(epochs,acc,'bo',label='Training Acc')\n",
    "plt.plot(epochs,val_acc,'r',label='Validation acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13999/13999 [==============================] - 0s 18us/step\n",
      "Loss 0.29455378696804635\n",
      "Accu 0.9206371883791135\n"
     ]
    }
   ],
   "source": [
    "Loss, accu=model.evaluate(TestData,TestTarget)\n",
    "print(\"Loss\",Loss)\n",
    "print(\"Accu\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 1111  Correct :12888\n",
      "Testing Accuracy: 92.0637188370598\n",
      "Testing Erms: 0.2531278842149712\n"
     ]
    }
   ],
   "source": [
    "total=0.0\n",
    "counter=0\n",
    "for i in range(0,len(predictedTestTarget)):\n",
    "    total+=math.pow((TestTarget[i]-predictedTestTarget[i]),2)\n",
    "    #Count number of match\n",
    "    if (int(np.around(predictedTestTarget[i])) == TestTarget[i]):\n",
    "        counter+=1\n",
    "Accuracy=(counter*100/len(predictedTestTarget))\n",
    "Erms=math.sqrt(total/len(predictedTestTarget))\n",
    "\n",
    "print(\"Errors: \" + str(len(predictedTestTarget)-counter), \" Correct :\" + str(counter))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(Accuracy))\n",
    "print(\"Testing Erms: \" + str(Erms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
